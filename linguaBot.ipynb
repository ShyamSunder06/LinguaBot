{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f639379-2323-4982-83d8-f6b0163ae043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "import speech_recognition as sr\n",
    "import pyttsx3\n",
    "import language_tool_python\n",
    "from deep_translator import GoogleTranslator\n",
    "from langdetect import detect\n",
    "from PIL import Image\n",
    "import os\n",
    "import logging\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import cohere\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Initialize Cohere AI (Replace with your API Key)\n",
    "COHERE_API_KEY = \"BBOt2HXFWB0r6mhkkEJpSIyG9wGt3LWz4HZdLzlo\"  # Replace with your actual API key\n",
    "co = cohere.Client(COHERE_API_KEY)\n",
    "\n",
    "# Initialize AI-based image captioning model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "    logging.info(\"BLIP model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error loading BLIP model: {e}\")\n",
    "    blip_processor, blip_model = None, None\n",
    "\n",
    "# Initialize grammar checker\n",
    "try:\n",
    "    tool = language_tool_python.LanguageTool(\"en-US\")\n",
    "    logging.info(\"Grammar checker initialized.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error initializing LanguageTool: {e}\")\n",
    "    tool = None\n",
    "\n",
    "# Global variables for speech\n",
    "is_speaking = False\n",
    "\n",
    "# ‚úÖ Multilingual Text-to-Speech (TTS)\n",
    "def speak(text, lang=\"en\"):\n",
    "    \"\"\"Convert text to speech in the detected language.\"\"\"\n",
    "    try:\n",
    "        # Create a new engine instance for each call\n",
    "        local_engine = pyttsx3.init()\n",
    "        local_engine.setProperty(\"rate\", 150)\n",
    "        local_engine.setProperty(\"volume\", 1.0)\n",
    "        \n",
    "        # Translate text if needed\n",
    "        if lang not in [\"en\", \"ta\", \"te\"]:\n",
    "            text = GoogleTranslator(source=\"auto\", target=lang).translate(text)\n",
    "\n",
    "        if lang == \"ta\":\n",
    "            local_engine.setProperty(\"voice\", \"tam\")  # Tamil voice (if available)\n",
    "        elif lang == \"te\":\n",
    "            local_engine.setProperty(\"voice\", \"tel\")  # Telugu voice (if available)\n",
    "\n",
    "        local_engine.say(text)\n",
    "        local_engine.runAndWait()\n",
    "        local_engine.stop()  # Stop the engine after speaking\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Speech error: {e}\")\n",
    "\n",
    "# ‚úÖ Multilingual Speech Recognition\n",
    "def listen():\n",
    "    \"\"\"Capture user input via microphone, detect language, and convert to text.\"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "    recognizer.energy_threshold = 300  \n",
    "\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Listening...\")\n",
    "        recognizer.adjust_for_ambient_noise(source, duration=1)\n",
    "\n",
    "        try:\n",
    "            audio = recognizer.listen(source, timeout=5)\n",
    "            text = recognizer.recognize_google(audio, language=\"en-IN\")  # Supports Indian languages\n",
    "            detected_language = detect(text)  # Detect language\n",
    "            print(f\"User said ({detected_language}): {text}\")\n",
    "            return text.lower(), detected_language\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"I couldn't understand that.\")\n",
    "            return None, \"en\"\n",
    "        except sr.RequestError:\n",
    "            print(\"Speech recognition service is unavailable.\")\n",
    "            return None, \"en\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in voice recognition: {e}\")\n",
    "            return None, \"en\"\n",
    "\n",
    "# ‚úÖ AI-Powered Response Generation\n",
    "def generate_ai_response(prompt, lang=\"en\"):\n",
    "    \"\"\"Generate AI-powered responses using Cohere and translate them.\"\"\"\n",
    "    try:\n",
    "        response = co.generate(\n",
    "            model=\"command\",\n",
    "            prompt=prompt,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        ai_text = response.generations[0].text.strip()\n",
    "        \n",
    "        # Translate response if needed\n",
    "        if lang in [\"ta\", \"te\"]:\n",
    "            ai_text = GoogleTranslator(source=\"auto\", target=lang).translate(ai_text)\n",
    "\n",
    "        return ai_text\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating AI response: {e}\")\n",
    "        return \"I'm sorry, I couldn't generate a response.\"\n",
    "\n",
    "# ‚úÖ OCR for Text Extraction\n",
    "def extract_text(image_path):\n",
    "    \"\"\"Extract text from an image using OCR.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(image_path):\n",
    "            return \"Image not found.\", \"en\"\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image).strip()\n",
    "        detected_language = detect(text) if text else \"en\"\n",
    "        logging.info(f\"Extracted text ({detected_language}): {text}\")\n",
    "        return text if text else \"No text detected.\", detected_language\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text: {e}\")\n",
    "        return \"Failed to extract text.\", \"en\"\n",
    "# ‚úÖ AI Image Captioning\n",
    "def generate_detailed_caption(image_path):\n",
    "    \"\"\"Generate detailed captions using BLIP and GPT.\"\"\"\n",
    "    if not blip_processor or not blip_model:\n",
    "        return \"Image captioning model is unavailable.\", \"en\"\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(image_path):\n",
    "            return \"Image not found.\", \"en\"\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = blip_processor(image, return_tensors=\"pt\").to(device)\n",
    "        caption = blip_model.generate(**inputs)\n",
    "        caption_text = blip_processor.batch_decode(caption, skip_special_tokens=True)[0]\n",
    "\n",
    "        detected_language = detect(caption_text)\n",
    "        logging.info(f\"Generated caption ({detected_language}): {caption_text}\")\n",
    "\n",
    "        return caption_text, detected_language\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error generating detailed caption: {e}\")\n",
    "        return \"Failed to generate detailed image caption.\", \"en\"\n",
    "\n",
    "# ‚úÖ Chatbot Logic\n",
    "def chatbot(image_path=None):\n",
    "    \"\"\"Chatbot that processes images and engages in a conversation.\"\"\"\n",
    "    image_context = {\n",
    "        \"caption\": None,\n",
    "        \"caption_lang\": \"en\",\n",
    "        \"extracted_text\": None,\n",
    "        \"text_lang\": \"en\"\n",
    "    }\n",
    "\n",
    "    if image_path:\n",
    "        print(\"\\nüîç Processing Image...\")\n",
    "        image_context[\"caption\"], image_context[\"caption_lang\"] = generate_detailed_caption(image_path)\n",
    "        image_context[\"extracted_text\"], image_context[\"text_lang\"] = extract_text(image_path)\n",
    "        print(f\"\\nüñº Caption: {image_context['caption']} ({image_context['caption_lang']})\")\n",
    "        print(f\"\\nüìú Extracted Text: {image_context['extracted_text']} ({image_context['text_lang']})\")\n",
    "\n",
    "    while True:\n",
    "        user_input, user_lang = listen()\n",
    "        if user_input == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            speak(\"Goodbye!\", user_lang)\n",
    "            break\n",
    "\n",
    "        if image_context[\"caption\"]:\n",
    "            prompt = f\"Based on the image description: {image_context['caption']}, answer: {user_input}\"\n",
    "            response = generate_ai_response(prompt, user_lang)\n",
    "            print(f\"\\nü§ñ AI ({user_lang}): {response}\")\n",
    "            speak(response, user_lang)\n",
    "        else:\n",
    "            response = generate_ai_response(user_input, user_lang)\n",
    "            print(f\"\\nü§ñ AI ({user_lang}): {response}\")\n",
    "            speak(response, user_lang)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = input(\"Enter the image path (or leave empty to skip image analysis): \").strip()\n",
    "    chatbot(image_path if image_path else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6feabb-0b14-4835-9441-0b0c317d7108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
